{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowpark Python - TPC DS  - Customer Lifetime Value\n",
    "\n",
    "This demo utilizes the [TPC DS sample](https://docs.snowflake.com/en/user-guide/sample-data-tpcds.html) dataset that is made available via  Snowflake share. It can be configured to run on either the 10 TB or the 100 TB version of the dataset. \n",
    "\n",
    "This illustrates how to utilize Snowpark for feature engineering, training, inference, streams, and tasks to answer a common question for retailers: What is the value of a customer across all sales channels? \n",
    "\n",
    "We will use sales data, customer information, and demographic data to see if we can accurately predict the lifetime value of a customer.\n",
    "\n",
    "In the repo you can also find a streamlit app showing how to connect to snowflake as a data source.\n",
    "Streamlit will soon be available in Snowsight as well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup:\n",
    "The TPC DS data is available already to you in your Snowflake account as shared database utlizing Snowflake's data sharing. This means you as the user will never incur the costs of storing this large dataset. \n",
    "\n",
    " 1. Create a conda environment using the provided *environment.yml* file. \n",
    "    1. `conda env create -f environment.yml `\n",
    "    2. Activate that created conda environment by `conda activate snowpark_ml_test`\n",
    " 2. Edit the *creds.json* file to with your account information to connect to your account. \n",
    " 3. Load Jupyter or equivalent notebook to begin executing the notebook. \n",
    " 4. Refer to creds(example).json on how to configure login credentials\n",
    "\n",
    "### Table of Contents:\n",
    "1. Connecting to Snowflake\n",
    "2. Environment Setup\n",
    "3. Feature Engineering\n",
    "4. Model Training\n",
    "5. Infer Predictions\n",
    "6. Streams and Tasks\n",
    "\n",
    "### Cost Performance\n",
    "\n",
    "Below is a table of some observed performance stats I have observed in AWS US East Ohio. All times reported in seconds and assuming enterprise edition list pricing. \n",
    "\n",
    "| Dataset       \t| Data prep/Feature Eng Warehouse \t| Snowpark Optimized Warehouse \t| Time for feature eng/prep \t| Cost for feature eng/prep \t| Time for training \t| Cost for training \t| Time for inference \t| Cost for inference \t|\n",
    "|---------------\t|---------------------------------\t|------------------------------\t|---------------------------\t|---------------------------\t|-------------------\t|-------------------\t|--------------------\t|--------------------\t|\n",
    "| TPC-DS 10 TB  \t| 3XL                             \t| Medium                       \t| 60                        \t| $3.20                     \t| 1400.4            \t| $7.07             \t| 9.8                \t| $0.52              \t|\n",
    "| TPC-DS 100 TB \t| 3XL                             \t| Medium                       \t| 311.6                     \t| $16.51                    \t| 2210              \t| $11.05            \t| 24.6               \t| $1.30              \t|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.snowpark\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark import version as v\n",
    "from snowflake.snowpark.types import (\n",
    "    Variant,\n",
    "    IntegerType,\n",
    "    BooleanType,\n",
    "    FloatType,\n",
    "    StringType,\n",
    "    DoubleType,\n",
    "    BooleanType,\n",
    "    DateType,\n",
    "    StructType,\n",
    "    StructField\n",
    ")\n",
    "from snowflake.snowpark.functions import (\n",
    "    udtf,\n",
    "    udf,\n",
    "    col,\n",
    "    lit,\n",
    "    row_number,\n",
    "    table_function\n",
    ")\n",
    "import json \n",
    "\n",
    "#See creds.json for an example on how to build your connection string.\n",
    "with open('connection.json') as f:\n",
    "    data = json.load(f)\n",
    "    USERNAME = data['user']\n",
    "    PASSWORD = data['password']\n",
    "    SF_ACCOUNT = data['account']\n",
    "    SF_WH = data['warehouse']\n",
    "    SF_ROLE = data['role']\n",
    "\n",
    "CONNECTION_PARAMETERS = {\n",
    "   \"account\": SF_ACCOUNT,\n",
    "   \"user\": USERNAME,\n",
    "   \"password\": PASSWORD,\n",
    "   \"role\": SF_ROLE\n",
    "}\n",
    "\n",
    "session = Session.builder.configs(CONNECTION_PARAMETERS).create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using data marketplace, copy down sample_data into snowflake. This can be done with any data on the marketplace made available to your account\n",
    "session.use_role('ACCOUNTADMIN')\n",
    "session.sql('''create database if not exists snowflake_sample_data from share sfc_samples.sample_data''').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create the context for our demo. \n",
    "# Note that I am going to use a 3X-Large warehouse for the demo for the sake of time.\n",
    "# By commenting you can swap between 3XL and XL\n",
    "# refer to the readme for cost comparison of warehousing sizing for the demo.\n",
    "\n",
    "session.sql('CREATE DATABASE IF NOT EXISTS tpcds_xgboost').collect()\n",
    "session.sql('CREATE SCHEMA IF NOT EXISTS tpcds_xgboost.demo').collect()\n",
    "session.sql(\"create or replace warehouse FE_AND_INFERENCE_WH with warehouse_size='3X-LARGE' auto_suspend=60\").collect()\n",
    "# session.sql(\"create or replace warehouse FE_AND_INFERENCE_WH with warehouse_size='X-LARGE' auto_suspend=60\").collect()\n",
    "session.sql(\"create or replace warehouse snowpark_opt_wh with warehouse_size = 'MEDIUM' warehouse_type = 'SNOWPARK-OPTIMIZED'\").collect()\n",
    "session.sql(\"alter warehouse snowpark_opt_wh set max_concurrency_level = 1\").collect()\n",
    "session.use_warehouse('FE_AND_INFERENCE_WH')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select either 100 or 10 for the TPC-DS Dataset size to use below. See (https://docs.snowflake.com/en/user-guide/sample-data-tpcds.html)[here] for more information If you choose 100, I recommend >= 3XL warehouse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting either the 10TB or 100TB set of sample data\n",
    "TPCDS_SIZE_PARAM = 10\n",
    "SNOWFLAKE_SAMPLE_DB = 'SNOWFLAKE_SAMPLE_DATA' # Name of Snowflake Sample Database might be different...\n",
    "\n",
    "if TPCDS_SIZE_PARAM == 100: \n",
    "    TPCDS_SCHEMA = 'TPCDS_SF100TCL'\n",
    "elif TPCDS_SIZE_PARAM == 10:\n",
    "    TPCDS_SCHEMA = 'TPCDS_SF10TCL'\n",
    "else:\n",
    "    raise ValueError(\"Invalid TPCDS_SIZE_PARAM selection\")\n",
    "\n",
    "# read the Snowflake marketplace tables into Snowpark dataframes\n",
    "store_sales = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.store_sales')\n",
    "catalog_sales = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.catalog_sales') \n",
    "web_sales = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.web_sales') \n",
    "date = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.date_dim')\n",
    "dim_stores = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.store')\n",
    "customer = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.customer')\n",
    "address = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.customer_address')\n",
    "demo = session.table(f'{SNOWFLAKE_SAMPLE_DB}.{TPCDS_SCHEMA}.customer_demographics')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering / Building Feature Table\n",
    "We will aggregate sales by customer across all channels(web, store, catalogue) and join that to customer demographic data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping all sales by customer and summing them. \n",
    "# note that F.sum() refers to the package snowflake.snowpark.functions\n",
    "# please see documentation here for functions in this library:\n",
    "# https://docs.snowflake.com/ko/developer-guide/snowpark/reference/scala/com/snowflake/snowpark/functions$.html\n",
    "\n",
    "store_sales_agged = store_sales.group_by('ss_customer_sk').agg(F.sum('ss_sales_price').as_('total_sales'))\n",
    "web_sales_agged = web_sales.group_by('ws_bill_customer_sk').agg(F.sum('ws_sales_price').as_('total_sales'))\n",
    "catalog_sales_agged = catalog_sales.group_by('cs_bill_customer_sk').agg(F.sum('cs_sales_price').as_('total_sales'))\n",
    "store_sales_agged = store_sales_agged.rename('ss_customer_sk', 'customer_sk')\n",
    "web_sales_agged = web_sales_agged.rename('ws_bill_customer_sk', 'customer_sk')\n",
    "catalog_sales_agged = catalog_sales_agged.rename('cs_bill_customer_sk', 'customer_sk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unioning the sales from the different sales channels into a single snowpark dataframe\n",
    "total_sales = store_sales_agged.union_all(web_sales_agged)\n",
    "total_sales = total_sales.union_all(catalog_sales_agged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vertically summing the sales from the unioned snowpark dataframe and grouping by customer\n",
    "total_sales = total_sales.group_by('customer_sk').agg(F.sum('total_sales').as_('total_sales'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query a subset of the customer information into a snowpark dataframe\n",
    "customer = customer.select('c_customer_sk','c_current_hdemo_sk', 'c_current_addr_sk', 'c_customer_id', 'c_birth_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the query results to a query subset from the address table\n",
    "customer = customer.join(address.select('ca_address_sk', 'ca_zip'), customer['c_current_addr_sk'] == address['ca_address_sk'] )\n",
    "# join the customer and address query results to demographic information from demo table\n",
    "customer = customer.join(demo.select('cd_demo_sk', 'cd_gender', 'cd_marital_status', 'cd_credit_rating', 'cd_education_status', 'cd_dep_count'),\n",
    "                                customer['c_current_hdemo_sk'] == demo['cd_demo_sk'] )\n",
    "# renaming customer_sk column\n",
    "customer = customer.rename('c_customer_sk', 'customer_sk')\n",
    "customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join our aggregated sales information to our customer information query results.\n",
    "final_df = total_sales.join(customer,'customer_sk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write this final snowpark dataframe back to snowflake\n",
    "session.use_database('tpcds_xgboost')\n",
    "session.use_schema('demo')\n",
    "final_df.write.mode('overwrite').save_as_table('feature_store')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the context for model training\n",
    "session.use_database('tpcds_xgboost')\n",
    "session.use_schema('demo')\n",
    "session.use_warehouse('snowpark_opt_wh')\n",
    "session.add_packages('snowflake-snowpark-python', 'scikit-learn', 'pandas', 'numpy', 'joblib', 'cachetools', 'xgboost', 'joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the stage\n",
    "session.sql('CREATE OR REPLACE STAGE ml_models ').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def train_model(session: snowflake.snowpark.Session) -> float:\n",
    "    # read the table we assembled in the prior section into a snowpark dataframe\n",
    "    snowdf = session.table(\"feature_store\")\n",
    "    # drop the non-predictor columns\n",
    "    snowdf = snowdf.drop(['CUSTOMER_SK', 'C_CURRENT_HDEMO_SK', 'C_CURRENT_ADDR_SK', 'C_CUSTOMER_ID', 'CA_ADDRESS_SK', 'CD_DEMO_SK'])\n",
    "    # split the data into test/train\n",
    "    snowdf_train, snowdf_test = snowdf.random_split([0.8, 0.2], seed=82) \n",
    "\n",
    "    # save the train and test sets as time stamped tables in Snowflake \n",
    "    snowdf_train.write.mode(\"overwrite\").save_as_table(\"tpcds_xgboost.demo.tpc_TRAIN\")\n",
    "    snowdf_test.write.mode(\"overwrite\").save_as_table(\"tpcds_xgboost.demo.tpc_TEST\")\n",
    "    # drop the outcome for training set\n",
    "    train_x = snowdf_train.drop(\"TOTAL_SALES\").to_pandas()\n",
    "    # select the outcome for training set\n",
    "    train_y = snowdf_train.select(\"TOTAL_SALES\").to_pandas()\n",
    "    # drop the outcome for test set\n",
    "    test_x = snowdf_test.drop(\"TOTAL_SALES\").to_pandas()\n",
    "    # select the outcome for test set\n",
    "    test_y = snowdf_test.select(\"TOTAL_SALES\").to_pandas()\n",
    "    # define the categorical columns for the pipeline feature engineering\n",
    "    cat_cols = ['CA_ZIP', 'CD_GENDER', 'CD_MARITAL_STATUS', 'CD_CREDIT_RATING', 'CD_EDUCATION_STATUS']\n",
    "    # define the numerical columns for the pipeline feature engineering\n",
    "    num_cols = ['C_BIRTH_YEAR', 'CD_DEP_COUNT']\n",
    "\n",
    "    # define the numerical portion of the pipeline\n",
    "    num_pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "            ('std_scaler', StandardScaler()),\n",
    "        ])\n",
    "\n",
    "    # define the pre-processor\n",
    "    preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', num_pipeline, num_cols),\n",
    "                  ('encoder', OneHotEncoder(handle_unknown=\"ignore\"), cat_cols) ])\n",
    "\n",
    "    # assemble the pipe with pre-processor and model\n",
    "    pipe = Pipeline([('preprocessor', preprocessor), \n",
    "                        ('xgboost', XGBRegressor())])\n",
    "    # fit the data with pre-processing and model\n",
    "    pipe.fit(train_x, train_y)\n",
    "\n",
    "    # predict the outcomes for test data\n",
    "    test_preds = pipe.predict(test_x)\n",
    "    # calculate the error between predictions and actuals for test data\n",
    "    rmse = mean_squared_error(test_y, test_preds)\n",
    "    # define path for pipe and model file\n",
    "    model_file = os.path.join('/tmp', 'model.joblib')\n",
    "    # dump model pipe and model to joblib file\n",
    "    joblib.dump(pipe, model_file)\n",
    "    # put the joblib file in the ml_models stage\n",
    "    session.file.put(model_file, \"@ml_models\",overwrite=True)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the stored procedure\n",
    "train_model_sp = F.sproc(train_model, session=session, replace=True)\n",
    "# Switch to Snowpark Optimized Warehouse for training and to run the stored proc\n",
    "session.use_warehouse('snowpark_opt_wh')\n",
    "# run the stored procedure\n",
    "train_model_sp(session=session)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer Predictions Using Stored Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch back to feature engineering/inference warehouse\n",
    "session.use_warehouse('FE_AND_INFERENCE_WH')\n",
    "session.use_database('TPCDS_XGBOOST')\n",
    "session.use_schema('DEMO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import cachetools\n",
    "import joblib\n",
    "from snowflake.snowpark import types as T\n",
    "\n",
    "# importing pipe file from stage into import directory\n",
    "session.add_import(\"@ml_models/model.joblib.gz\")\n",
    "\n",
    "# defining featurelist\n",
    "features = ['C_BIRTH_YEAR', 'CA_ZIP', 'CD_GENDER', 'CD_MARITAL_STATUS', 'CD_CREDIT_RATING', 'CD_EDUCATION_STATUS', 'CD_DEP_COUNT']\n",
    "\n",
    "# clearing the cache\n",
    "@cachetools.cached(cache={})\n",
    "\n",
    "def read_file(filename):\n",
    "       # snowflake import directory exists on snowflake\n",
    "       import_dir = sys._xoptions.get(\"snowflake_import_directory\")\n",
    "       \n",
    "       # reading the file pip into session\n",
    "       if import_dir:\n",
    "              with open(os.path.join(import_dir, filename), 'rb') as file:\n",
    "                     m = joblib.load(file)\n",
    "                     return m\n",
    "\n",
    "# creating the udf on snowflake. The @ symbol will take whatever function follows and attribute it to the udf.\n",
    "@F.pandas_udf(session=session, max_batch_size=10000, is_permanent=True, stage_location='@ml_models', name=\"clv_xgboost_udf\", replace=True)\n",
    "\n",
    "\n",
    "# naming and defining the UDF as predict\n",
    "def predict(df:  T.PandasDataFrame[int, str, str, str, str, str, int]) -> T.PandasSeries[float]:\n",
    "       m = read_file('model.joblib.gz')\n",
    "       # m = read_file(\"@mlmodels/model.joblib.gz\")\n",
    "\n",
    "       df.columns = features\n",
    "       \n",
    "       return m.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# read our prepared feature store table in our Snowflake account storage into Snowpark dataframe\n",
    "inference_df = session.table('feature_store')\n",
    "# drop the non-feature columns\n",
    "inference_df = inference_df.drop(['CUSTOMER_SK', 'C_CURRENT_HDEMO_SK', 'C_CURRENT_ADDR_SK', 'C_CUSTOMER_ID', 'CA_ADDRESS_SK', 'CD_DEMO_SK'])\n",
    "# drop the outcome\n",
    "inputs = inference_df.drop(\"TOTAL_SALES\")\n",
    "# call the udf for inference\n",
    "snowdf_results = inference_df.select(*inputs,\n",
    "                    predict(*inputs).alias('PREDICTION'), \n",
    "                    (F.col('TOTAL_SALES')).alias('ACTUAL_SALES')\n",
    "                    )\n",
    "# write the results to a table in snowflake\n",
    "snowdf_results.write.mode('overwrite').save_as_table('predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowdf_results.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNDER CONSTRUCTION UNDER HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streams and Tasks (Change Data Capture and Scheduling)\n",
    "- Still need to get metadeta to reset for update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete me, just for piecework to set context\n",
    "session.use_database('TPCDS_XGBOOST')\n",
    "session.use_schema('DEMO')\n",
    "session.use_warehouse('compute_wh')\n",
    "session.use_role('ACCOUNTADMIN')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create or replace TABLE ROI_PRED (\n",
    "\tSEARCH_ENGINE NUMBER(38,0),\n",
    "\tSOCIAL_MEDIA NUMBER(38,0),\n",
    "\tVIDEO NUMBER(38,0),\n",
    "\tEMAIL NUMBER(38,0),\n",
    "\tPREDICTED_ROI FLOAT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql('drop table feature_stage').collect()\n",
    "session.sql(\"\"\"\n",
    "\tcreate or replace TABLE feature_stage (\n",
    "\tCUSTOMER_SK VARCHAR(50),\n",
    "\tTOTAL_SALES NUMBER(38,2),\n",
    "\tC_CURRENT_HDEMO_SK VARCHAR(50),\n",
    "\tC_CURRENT_ADDR_SK VARCHAR(50),\n",
    "\tC_CUSTOMER_ID VARCHAR(50),\n",
    "\tC_BIRTH_YEAR NUMBER(38,0),\n",
    "\tCA_ADDRESS_SK NUMBER(38,0),\n",
    "\tCA_ZIP NUMBER(38,0),\n",
    "\tCD_DEMO_SK NUMBER(38,0),\n",
    "\tCD_GENDER VARCHAR(50),\n",
    "\tCD_MARITAL_STATUS VARCHAR(50),\n",
    "\tCD_CREDIT_RATING VARCHAR(50),\n",
    "    CD_EDUCATION_STATUS VARCHAR(50),\n",
    "\tCD_DEP_COUNT NUMBER(38,0),\n",
    "\tPREDICTION NUMBER(38,0)\n",
    ")\n",
    "\"\"\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"\"\"\n",
    "create or replace stream xg_boost_stream on table feature_stage;\n",
    "\"\"\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = ['C_BIRTH_YEAR', 'CA_ZIP', 'CD_GENDER', 'CD_MARITAL_STATUS', 'CD_CREDIT_RATING', 'CD_EDUCATION_STATUS', 'CD_DEP_COUNT']\n",
    "session.sql(\"select * from xg_boost_stream\").show()\n",
    "feature_store = session.table('feature_store')\n",
    "el_df = feature_store.select(\"*\").distinct().limit(100)\n",
    "el_df = el_df.withColumn('PREDICTION', F.lit(None).cast(DoubleType()))\n",
    "el_df.write.mode('append').save_as_table('feature_stage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"select * from xg_boost_stream\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read our prepared feature store table in our Snowflake account storage into Snowpark dataframe\n",
    "feature_stage = session.table('feature_stage')\n",
    "base_table = session.table('xg_boost_stream').filter((col('METADATA$ACTION') == 'INSERT') & col(\"PREDICTION\").isNull())\n",
    "base_table.show()\n",
    "\n",
    "# drop the non-feature columns\n",
    "inference_df = base_table.drop(['C_CURRENT_HDEMO_SK', 'C_CURRENT_ADDR_SK', 'C_CUSTOMER_ID', 'CA_ADDRESS_SK', 'CD_DEMO_SK', \"METADATA$ACTION\", \"METADATA$ISUPDATE\", \"METADATA$ROW_ID\"])\n",
    "# drop the outcome\n",
    "inputs = inference_df.drop([\"TOTAL_SALES\", \"CUSTOMER_SK\", \"PREDICTION\"])\n",
    "\n",
    "# call the udf for inference\n",
    "snowdf_results = feature_stage.select(*inputs,\n",
    "                    predict(*inputs).alias('PREDICTION'), \n",
    "                    (F.col('TOTAL_SALES')).alias('ACTUAL_SALES'),\n",
    "                    (F.col('CUSTOMER_SK')),\n",
    "                    (F.col('C_CURRENT_ADDR_SK')),\n",
    "                    F.col('C_CURRENT_HDEMO_SK'),\n",
    "                    F.col('C_CUSTOMER_ID'),\n",
    "                    F.col('CA_ADDRESS_SK'),\n",
    "                    F.col('CD_DEMO_SK'),\n",
    "                    )\n",
    "snowdf_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"ALTER SESSION SET ERROR_ON_NONDETERMINISTIC_MERGE = FALSE;\").collect()\n",
    "feature_stage.merge(snowdf_results, (feature_stage['CUSTOMER_SK'] == snowdf_results[\"CUSTOMER_SK\"]) & \\\n",
    "                                    (feature_stage[\"C_BIRTH_YEAR\"] == snowdf_results[\"C_BIRTH_YEAR\"]) & \\\n",
    "                                    (feature_stage['CA_ZIP'] == snowdf_results[\"CA_ZIP\"]) & \\\n",
    "                                    (feature_stage['CD_GENDER'] == snowdf_results[\"CD_GENDER\"]) & \\\n",
    "                                    (feature_stage['CD_MARITAL_STATUS'] == snowdf_results[\"CD_MARITAL_STATUS\"]) & \\\n",
    "                                    (feature_stage['CD_CREDIT_RATING'] == snowdf_results[\"CD_CREDIT_RATING\"]) & \\\n",
    "                                    (feature_stage['CD_EDUCATION_STATUS'] == snowdf_results[\"CD_EDUCATION_STATUS\"]) & \\\n",
    "                                    (feature_stage['CD_DEP_COUNT'] == snowdf_results[\"CD_DEP_COUNT\"]), \n",
    "                    [F.when_matched().update({\"PREDICTION\" : snowdf_results['PREDICTION']})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_stage.show()\n",
    "session.sql(\"select * from xg_boost_stream\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Aug 11 2023, 19:44:49) \n[Clang 15.0.0 (clang-1500.0.40.1)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
